{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import numpy as np\n",
        "\n",
        "class TextClassificationModel:\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        self.model = tf.keras.Sequential()\n",
        "        self.model.add(tf.keras.layers.Dense(512, activation='relu', input_shape=(input_shape,)))\n",
        "        self.model.add(tf.keras.layers.Dense(output_shape, activation='softmax'))\n",
        "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs=10, batch_size=32, verbose=1):\n",
        "        self.model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\n",
        "    def evaluate(self, X_val, y_val):\n",
        "        _, accuracy = self.model.evaluate(X_val, y_val)\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "class TextClassificationPipeline:\n",
        "    def __init__(self, path, input_column, output_column):\n",
        "        self.path = path\n",
        "        self.input_column = input_column\n",
        "        self.output_column = output_column\n",
        "\n",
        "    def input_dataset(self):\n",
        "        data = pd.read_csv(self.path, sep=',', low_memory=False)\n",
        "        df = pd.DataFrame(data)\n",
        "        return df\n",
        "\n",
        "    def data_cleaning(self, df):\n",
        "        df[self.input_column] = df[self.input_column].map(str)\n",
        "        df[self.input_column] = df[self.input_column].str.lower()\n",
        "        df[self.input_column] = df[self.input_column].str.replace(r'[^\\w\\s]+', ' ')\n",
        "        df[self.input_column] = df[self.input_column].str.replace(r'_+', ' ')\n",
        "        df[self.input_column] = df[self.input_column].str.replace('\\s+', ' ', regex=True)\n",
        "        return df\n",
        "\n",
        "    def input_output(self, df):\n",
        "        x = df.loc[:, self.input_column]\n",
        "        y = df.loc[:, self.output_column]\n",
        "        print('Data Divided Successfully Into Input & Output')\n",
        "        return x, y\n",
        "\n",
        "    def text_vectorization(self, x):\n",
        "        vectorizer = TextVectorization(max_tokens=10000, output_mode='count')\n",
        "        vectorizer.adapt(x.tolist())  # Convert to list\n",
        "        x_counts = vectorizer(x).numpy()\n",
        "        return x_counts\n",
        "\n",
        "    def label_mapping(self, y):\n",
        "        label_mapping = {label: index for index, label in enumerate(set(y))}\n",
        "        y = [label_mapping[label] for label in y]\n",
        "        return y\n",
        "\n",
        "    def split_data(self, X_counts, y, test_size=0.2, random_state=42):\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_counts, y, test_size=test_size, random_state=random_state)\n",
        "        return X_train, X_val, y_train, y_val\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        # Load the dataset\n",
        "        df = self.input_dataset()\n",
        "\n",
        "        # Data cleaning\n",
        "        cleaned_df = self.data_cleaning(df)\n",
        "\n",
        "        # Dividing data into input and output\n",
        "        x, y = self.input_output(cleaned_df)\n",
        "\n",
        "        # Create a TextVectorization layer\n",
        "        x_counts = self.text_vectorization(x)\n",
        "\n",
        "        # Convert labels to numerical values\n",
        "        y_mapped = self.label_mapping(y)\n",
        "\n",
        "        # Split the data into training and validation sets\n",
        "        X_train, X_val, y_train, y_val = self.split_data(x_counts, y_mapped)\n",
        "\n",
        "        # Convert data to numpy arrays\n",
        "        X_train = np.array(X_train)\n",
        "        X_val = np.array(X_val)\n",
        "\n",
        "pipeline = TextClassificationPipeline('Deskripsi_Permasalahan.csv', 'Uraian', 'Topik')\n",
        "pipeline.run_pipeline()"
      ],
      "metadata": {
        "id": "kJ6IZLxBVvXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f9b5817-67b9-4379-ba90-0db93da2acab"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-80ed23c82eda>:36: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[self.input_column] = df[self.input_column].str.replace(r'[^\\w\\s]+', ' ')\n",
            "<ipython-input-20-80ed23c82eda>:37: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[self.input_column] = df[self.input_column].str.replace(r'_+', ' ')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Divided Successfully Into Input & Output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model.h5\")\n",
        "print(\"Success\")"
      ],
      "metadata": {
        "id": "FJTwuNX0cryr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0749cd08-c212-4d77-8b13-c83d8e3b94f3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the saved model file\n",
        "files.download('model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "382ElhFadHeG",
        "outputId": "8a207d9f-82ab-4295-fea4-9e3a357fc584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7ca19e8d-3478-4fe1-8843-7ab84cbfb0f9\", \"Amati_Model.h5\", 4757512)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}